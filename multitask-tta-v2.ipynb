{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize, MaxAbsScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler,SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d1 = pd.read_json(\"domain1_train_data.json\", lines=True)\n",
    "train_d2 = pd.read_json(\"domain2_train_data.json\", lines=True)\n",
    "test = pd.read_json(\"test_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d1['text_length'] = train_d1['text'].apply(len)\n",
    "train_d2['text_length'] = train_d2['text'].apply(len)\n",
    "test['text_length'] = test['text'].apply(len)\n",
    "\n",
    "train_d1['unique_tokens'] = train_d1['text'].apply(lambda x: len(set(x)))\n",
    "train_d1['token_diversity'] = train_d1['unique_tokens'] / train_d1['text_length']\n",
    "\n",
    "train_d2['unique_tokens'] = train_d2['text'].apply(lambda x: len(set(x)))\n",
    "train_d2['token_diversity'] = train_d2['unique_tokens'] / train_d2['text_length']\n",
    "\n",
    "test['unique_tokens'] = test['text'].apply(lambda x: len(set(x)))\n",
    "test['token_diversity'] = test['unique_tokens'] / test['text_length']\n",
    "\n",
    "train_d1[\"domain\"] = 0\n",
    "train_d2[\"domain\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 17119 + 1\n",
    "\n",
    "merge = pd.concat([train_d1, train_d2], ignore_index=True)\n",
    "merge = shuffle(merge, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BoW from token indices\n",
    "def tokens_to_bow(texts, vocab_size):\n",
    "    rows, cols, counts = [], [], []\n",
    "    for row_idx, text in enumerate(texts):\n",
    "        token_counts = {}\n",
    "        for token in text:\n",
    "            token_counts[token] = token_counts.get(token, 0) + 1\n",
    "        for token, count in token_counts.items():\n",
    "            rows.append(row_idx)\n",
    "            cols.append(token)\n",
    "            counts.append(count)\n",
    "    return csr_matrix((counts, (rows, cols)), shape=(len(texts), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_ngram_bow(texts, vocab_size, n_list=(1,2,3), min_freq=3):\n",
    "    rows, cols, counts = [], [], []\n",
    "    next_id  = vocab_size\n",
    "    ngram_id = {}      # {tuple: id}\n",
    "    freq_cnt = Counter()\n",
    "\n",
    "    # 1st pass: count n‑gram freq\n",
    "    for tokens in texts:\n",
    "        for n in n_list[1:]:                 # skip unigram\n",
    "            for i in range(len(tokens)-n+1):\n",
    "                freq_cnt[tuple(tokens[i:i+n])] += 1\n",
    "\n",
    "    # 2nd pass: build sparse rows\n",
    "    for r, tokens in enumerate(texts):\n",
    "        token_counts = Counter(tokens)       # unigram\n",
    "        # add n‑gram\n",
    "        for n in n_list[1:]:\n",
    "            for i in range(len(tokens)-n+1):\n",
    "                t = tuple(tokens[i:i+n])\n",
    "                if freq_cnt[t] >= min_freq:\n",
    "                    if t not in ngram_id:\n",
    "                        ngram_id[t] = next_id\n",
    "                        next_id += 1\n",
    "                    token_counts[ngram_id[t]] += 1\n",
    "        for c, cnt in token_counts.items():\n",
    "            rows.append(r); cols.append(c); counts.append(cnt)\n",
    "\n",
    "    X = csr_matrix((counts,(rows,cols)), shape=(len(texts), next_id))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_numeric_features(text_series, token_freq, rare_th=5):\n",
    "    \"\"\"\n",
    "    Return: ndarray shape (n_samples, n_features)\n",
    "            columns = [seq_len, mean_token_id, rare_ratio, token_diversity]\n",
    "    \"\"\"\n",
    "    # 序列长度 (sequence length)\n",
    "    seq_len = text_series.apply(len).values\n",
    "    \n",
    "    # 平均 token 索引 (mean token id)\n",
    "    mean_token_id = text_series.apply(lambda ts: np.mean(ts) if len(ts) > 0 else 0).values\n",
    "    \n",
    "    # 罕见词占比 (rare_token_ratio)\n",
    "    rare_ratio = text_series.apply(\n",
    "        lambda ts: sum(token_freq[t] < rare_th for t in ts) / len(ts) if len(ts) > 0 else 0\n",
    "    ).values\n",
    "\n",
    "    # # num unique tokens in sentences\n",
    "    # token_uniq = text_series.apply(lambda x: len(set(x))).values\n",
    "    \n",
    "    # 词多样性 (token_diversity = unique / length)\n",
    "    token_div = text_series.apply(\n",
    "        lambda ts: len(set(ts)) / len(ts) if len(ts) > 0 else 0\n",
    "    ).values\n",
    "    \n",
    "    feats = np.vstack([seq_len, token_div]).T\n",
    "    return feats.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token frequency percentiles and variance metrics\n",
    "def build_frequency_features(text_series, token_freq):\n",
    "    \"\"\"\n",
    "    Extract token frequency percentiles and variance metrics\n",
    "    \n",
    "    Returns:\n",
    "        ndarray shape (n_samples, 6) with columns:\n",
    "        [min_freq, 25th_pct, median_freq, 75th_pct, max_freq, freq_variance]\n",
    "    \"\"\"\n",
    "    # Calculate frequency-based features for each text\n",
    "    frequency_features = []\n",
    "    \n",
    "    for tokens in text_series:\n",
    "        if not tokens:\n",
    "            frequency_features.append([0, 0, 0, 0, 0, 0])\n",
    "            continue\n",
    "            \n",
    "        # Get frequencies for all tokens in this document\n",
    "        freqs = [token_freq[t] for t in tokens]\n",
    "        \n",
    "        # Calculate percentiles and variance\n",
    "        min_freq = min(freqs) if freqs else 0\n",
    "        q1 = np.percentile(freqs, 25)\n",
    "        median = np.percentile(freqs, 50)\n",
    "        q3 = np.percentile(freqs, 75)\n",
    "        max_freq = max(freqs) if freqs else 0\n",
    "        variance = np.var(freqs) if len(freqs) > 1 else 0\n",
    "        \n",
    "        frequency_features.append([min_freq, q1, median, q3, max_freq, variance])\n",
    "    \n",
    "    return np.array(frequency_features, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 train 数据，避免信息泄漏 (data leakage)\n",
    "token_freq = Counter()\n",
    "for tokens in pd.concat([train_d1, train_d2], ignore_index=True)[\"text\"]:\n",
    "    token_freq.update(tokens)\n",
    "\n",
    "# 0. 拼接所有文本\n",
    "all_texts = pd.concat([merge[\"text\"], test[\"text\"]], ignore_index=True)\n",
    "\n",
    "# 1. 一次性生成 n‑gram BoW\n",
    "X_all = tokens_to_ngram_bow(all_texts, vocab_size, n_list=(1,2,3), min_freq=3)\n",
    "\n",
    "# 2. 切分回 train / test\n",
    "X_bow   = X_all[:len(merge)]\n",
    "test_bow = X_all[len(merge):]\n",
    "\n",
    "# 3. Apply TF-IDF transformation\n",
    "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_bow)\n",
    "X_test_tfidf = tfidf_transformer.transform(test_bow)\n",
    "\n",
    "# 4. Seq length and token diversity features\n",
    "num_feats_train = build_numeric_features(merge[\"text\"], token_freq)   # ndarray\n",
    "num_feats_test  = build_numeric_features(test[\"text\"],  token_freq)\n",
    "\n",
    "# 5. Token frequency percentiles and variance\n",
    "train_freq_feats = build_frequency_features(merge[\"text\"], token_freq)\n",
    "test_freq_feats = build_frequency_features(test[\"text\"], token_freq)\n",
    "\n",
    "# 6. Scale numerical features\n",
    "num_scaler = MaxAbsScaler()\n",
    "train_num_feats_scaled = num_scaler.fit_transform(num_feats_train)\n",
    "test_num_feats_scaled = num_scaler.transform(num_feats_test)\n",
    "\n",
    "freq_scaler = MaxAbsScaler()\n",
    "train_freq_feats_scaled = freq_scaler.fit_transform(train_freq_feats)\n",
    "test_freq_feats_scaled = freq_scaler.transform(test_freq_feats)\n",
    "\n",
    "# 7. Concatenate all features for train and test separately\n",
    "X_train_final = hstack([\n",
    "    X_bow,\n",
    "    # X_train_tfidf,\n",
    "    train_num_feats_scaled,\n",
    "    # train_freq_feats_scaled\n",
    "]).tocsr()\n",
    "\n",
    "X_test_final = hstack([\n",
    "    test_bow,\n",
    "    # X_test_tfidf,\n",
    "    test_num_feats_scaled,\n",
    "    # test_freq_feats_scaled\n",
    "]).tocsr()\n",
    "\n",
    "y = merge[\"domain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_similarity(X_test, X_train, y_train, k=3):\n",
    "    # Calculate cosine similarity between each test and train instance\n",
    "    # This will give a matrix of shape (n_test, n_train)\n",
    "    similarity_matrix = cosine_similarity(X_test, X_train)\n",
    "    \n",
    "    # For each test instance, find the indices of the k most similar training instances\n",
    "    # argsort sorts in ascending order, so we take the last k elements\n",
    "    top_k_indices = np.argsort(similarity_matrix, axis=1)[:, -k:]\n",
    "    \n",
    "    mean_predictions = np.array([\n",
    "        np.mean(y_train.iloc[indices]) for indices in top_k_indices\n",
    "    ])\n",
    "    \n",
    "    return mean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_by_bm25_simple(X_test, X_train, y_train, k=3):\n",
    "    # Convert sparse matrices to lists of indices where values are non-zero\n",
    "    train_docs = [X_train[i].nonzero()[1] for i in range(X_train.shape[0])]\n",
    "    test_docs = [X_test[i].nonzero()[1] for i in range(X_test.shape[0])]\n",
    "    \n",
    "    # Create BM25 object\n",
    "    bm25 = BM25Okapi(train_docs)\n",
    "    \n",
    "    # For each test document, get BM25 scores for all training documents\n",
    "    top_k_indices = []\n",
    "    for test_doc in test_docs:\n",
    "        scores = bm25.get_scores(test_doc)\n",
    "        top_indices = np.argsort(scores)[-k:]\n",
    "        top_k_indices.append(top_indices)\n",
    "    \n",
    "    mean_predictions = np.array([\n",
    "        np.mean(y_train.iloc[indices]) for indices in top_k_indices\n",
    "    ])\n",
    "    \n",
    "    return mean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_tta(text_indices, swap_prob=0.1, del_prob=0.05, insert_prob=0.1):\n",
    "    \"\"\"\n",
    "    Apply test-time augmentation to token indices with specified probabilities\n",
    "    \"\"\"\n",
    "    augmented = text_indices.copy()\n",
    "    n = len(augmented)\n",
    "    \n",
    "    # Random Swapping\n",
    "    if n >= 2:\n",
    "        for i in range(n-1):\n",
    "            if np.random.random() < swap_prob:\n",
    "                augmented[i], augmented[i+1] = augmented[i+1], augmented[i]\n",
    "    \n",
    "    # Random Deletion\n",
    "    if n > 0:\n",
    "        mask = np.random.random(n) > del_prob\n",
    "        augmented = augmented[mask]\n",
    "    \n",
    "    # Random Insertion\n",
    "    if n > 0 and len(augmented) > 0:\n",
    "        insertions = int(insert_prob * n)\n",
    "        for _ in range(insertions):\n",
    "            if len(augmented) == 0:\n",
    "                break\n",
    "            # Insert a copy of random existing token\n",
    "            pos = np.random.randint(0, len(augmented)+1)\n",
    "            token = np.random.choice(augmented)\n",
    "            augmented = np.insert(augmented, pos, token)\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "def apply_tta(X_test, n_augmentations=5):\n",
    "    \"\"\"\n",
    "    Apply test-time augmentation to generate multiple augmented versions\n",
    "    \"\"\"\n",
    "    test_indices_list = [X_test[i].nonzero()[1] for i in range(X_test.shape[0])]\n",
    "    \n",
    "    augmented_test_matrices = []\n",
    "    \n",
    "    for _ in range(n_augmentations):\n",
    "        aug_indices = []\n",
    "        for indices in test_indices_list:\n",
    "            # Apply augmentation with conservative probabilities\n",
    "            aug = augment_tta(\n",
    "                indices,\n",
    "                swap_prob=0.05,  # 5% swap probability\n",
    "                del_prob=0.05,   # 5% deletion probability\n",
    "                insert_prob=0.05 # 5% insertion probability\n",
    "            )\n",
    "            aug_indices.append(aug)\n",
    "        \n",
    "        # Convert back to sparse matrix\n",
    "        rows, cols, counts = [], [], []\n",
    "        for row_idx, indices in enumerate(aug_indices):\n",
    "            for token in indices:\n",
    "                rows.append(row_idx)\n",
    "                cols.append(token)\n",
    "                counts.append(1)\n",
    "        \n",
    "        X_aug = csr_matrix((counts, (rows, cols)), \n",
    "                          shape=(len(aug_indices), X_test.shape[1]))\n",
    "        augmented_test_matrices.append(X_aug)\n",
    "    \n",
    "    return augmented_test_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## out of fold predictions\n",
    "oof_preds = np.zeros(len(y), dtype=int)\n",
    "oof_probs = np.zeros(len(y))\n",
    "\n",
    "test_preds = np.zeros(len(test))\n",
    "\n",
    "FOLDS = 20\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for train_index, val_index in skf.split(X_train_final, y):\n",
    "    X_train = X_train_final[train_index]\n",
    "    X_val = X_train_final[val_index]\n",
    "\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_val = y.iloc[val_index]\n",
    "\n",
    "    smote = SMOTE(random_state=42, sampling_strategy=0.6)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Model prediction\n",
    "    clf1 = LogisticRegression(max_iter=1000, C=0.1, solver='liblinear', random_state=42)\n",
    "    clf2 = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\", alpha=0.1, penalty='l2')\n",
    "    # clf3 = RandomForestClassifier(n_estimators=200) \n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('sgd', clf2)], voting='soft')\n",
    "\n",
    "    eclf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # VALIDATION: Apply TTA to validation set\n",
    "    X_val_aug_list = apply_tta(X_val, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented validation versions\n",
    "    val_probs = np.zeros(X_val.shape[0])\n",
    "    for X_val_aug in X_val_aug_list:\n",
    "        val_probs += eclf.predict_proba(X_val_aug)[:, 1]\n",
    "    val_probs /= len(X_val_aug_list)\n",
    "    \n",
    "    oof_preds[val_index] = (val_probs > 0.5).astype(int)\n",
    "\n",
    "    # Apply TTA with 5 augmentations\n",
    "    X_test_aug_list = apply_tta(X_test_final, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented versions\n",
    "    test_probs = np.zeros(len(test))\n",
    "    for X_test_aug in X_test_aug_list:\n",
    "        test_probs += eclf.predict_proba(X_test_aug)[:, 1]\n",
    "    test_probs /= len(X_test_aug_list)\n",
    "\n",
    "    # # direct output for valication data, probability for test for mean value later\n",
    "    # oof_preds[val_index] = eclf.predict(X_val)\n",
    "    # oof_probs[val_index] += eclf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # use model for test prediction\n",
    "    test_preds += test_probs\n",
    "\n",
    "test_preds /= FOLDS # Average across folds\n",
    "\n",
    "test_labels = (test_preds > 0.5).astype(int)\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall OOF accuracy with model pred: 0.9485\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y, oof_preds)\n",
    "print(f\"Overall OOF accuracy with model pred: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.92      0.86      1000\n",
      "           1       0.98      0.95      0.97      5000\n",
      "\n",
      "    accuracy                           0.95      6000\n",
      "   macro avg       0.89      0.94      0.91      6000\n",
      "weighted avg       0.95      0.95      0.95      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, oof_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"domain_pred\"] = test_labels\n",
    "\n",
    "test_d1 = test[test['domain_pred'] == 0].copy()\n",
    "test_d2 = test[test['domain_pred'] == 1].copy()\n",
    "\n",
    "test_d1.reset_index(drop=True, inplace=True)\n",
    "test_d2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 train 数据，避免信息泄漏 (data leakage)\n",
    "token_freq_d1 = Counter()\n",
    "for tokens in train_d1[\"text\"]:\n",
    "    token_freq_d1.update(tokens)\n",
    "\n",
    "# 0. 拼接所有文本\n",
    "d1_texts = pd.concat([train_d1[\"text\"], test_d1[\"text\"]], ignore_index=True)\n",
    "\n",
    "# 1. 一次性生成 n‑gram BoW\n",
    "X_d1_all = tokens_to_ngram_bow(d1_texts, vocab_size, n_list=(1,2,3), min_freq=3)\n",
    "\n",
    "# 2. 切分回 train / test\n",
    "X_d1_bow   = X_d1_all[:len(train_d1)]\n",
    "test_d1_bow = X_d1_all[len(train_d1):]\n",
    "\n",
    "# 3. Apply TF-IDF transformation\n",
    "d1_tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "X_train_d1_tfidf = d1_tfidf_transformer.fit_transform(X_d1_bow)\n",
    "X_test_d1_tfidf = d1_tfidf_transformer.transform(test_d1_bow)\n",
    "\n",
    "# 4. Seq length and token diversity features\n",
    "num_feats_train_d1 = build_numeric_features(train_d1[\"text\"], token_freq_d1)   # ndarray\n",
    "num_feats_test_d1  = build_numeric_features(test_d1[\"text\"],  token_freq_d1)\n",
    "\n",
    "# 5. Token frequency percentiles and variance\n",
    "train_freq_feats_d1 = build_frequency_features(train_d1[\"text\"], token_freq_d1)\n",
    "test_freq_feats_d1 = build_frequency_features(test_d1[\"text\"], token_freq_d1)\n",
    "\n",
    "# 6. Scale numerical features\n",
    "d1_num_scaler = MaxAbsScaler()\n",
    "train_num_feats_scaled_d1 = d1_num_scaler.fit_transform(num_feats_train_d1)\n",
    "test_num_feats_scaled_d1 = d1_num_scaler.transform(num_feats_test_d1)\n",
    "\n",
    "d1_freq_scaler = MaxAbsScaler()\n",
    "train_freq_feats_scaled_d1 = d1_freq_scaler.fit_transform(train_freq_feats_d1)\n",
    "test_freq_feats_scaled_d1 = d1_freq_scaler.transform(test_freq_feats_d1)\n",
    "\n",
    "# 7. Concatenate all features for train and test separately\n",
    "X_train_final_d1 = hstack([\n",
    "    X_d1_bow,\n",
    "    # X_train_d1_tfidf,\n",
    "    train_num_feats_scaled_d1,\n",
    "    # train_freq_feats_scaled_d1\n",
    "]).tocsr()\n",
    "\n",
    "X_test_final_d1 = hstack([\n",
    "    test_d1_bow,\n",
    "    # X_test_d1_tfidf,\n",
    "    test_num_feats_scaled_d1,\n",
    "    # test_freq_feats_scaled_d1\n",
    "]).tocsr()\n",
    "\n",
    "y_d1 = train_d1[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## out of fold predictions\n",
    "oof_d1_preds = np.zeros(len(y_d1), dtype=int)\n",
    "oof_d1_probs = np.zeros(len(y_d1))\n",
    "\n",
    "test_d1_preds = np.zeros(len(test_d1))\n",
    "\n",
    "FOLDS = 20\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for train_index, val_index in skf.split(X_train_final_d1, y_d1):\n",
    "    X_train = X_train_final_d1[train_index]\n",
    "    X_val = X_train_final_d1[val_index]\n",
    "\n",
    "    y_train = y_d1.iloc[train_index]\n",
    "    y_val = y_d1.iloc[val_index]\n",
    "\n",
    "    # Model prediction\n",
    "    clf1 = LogisticRegression(max_iter=1000, C=0.1, solver='liblinear', random_state=42)\n",
    "    clf2 = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\", alpha=0.1, penalty='l2')\n",
    "    # clf3 = RandomForestClassifier(n_estimators=200) \n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('sgd', clf2)], voting='soft')\n",
    "\n",
    "    eclf.fit(X_train, y_train)\n",
    "\n",
    "    # VALIDATION: Apply TTA to validation set\n",
    "    X_val_aug_list = apply_tta(X_val, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented validation versions\n",
    "    val_probs = np.zeros(X_val.shape[0])\n",
    "    for X_val_aug in X_val_aug_list:\n",
    "        val_probs += eclf.predict_proba(X_val_aug)[:, 1]\n",
    "    val_probs /= len(X_val_aug_list)\n",
    "    \n",
    "    oof_d1_preds[val_index] = (val_probs > 0.5).astype(int)\n",
    "\n",
    "    # Apply TTA with 5 augmentations\n",
    "    X_test_aug_list = apply_tta(X_test_final_d1, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented versions\n",
    "    test_probs = np.zeros(len(test_d1))\n",
    "    for X_test_aug in X_test_aug_list:\n",
    "        test_probs += eclf.predict_proba(X_test_aug)[:, 1]\n",
    "    test_probs /= len(X_test_aug_list)\n",
    "\n",
    "    # # direct output for valication data, probability for test for mean value later\n",
    "    # oof_d1_preds[val_index] = eclf.predict(X_val)\n",
    "    # oof_d1_probs[val_index] += eclf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # use model for test prediction\n",
    "    test_d1_preds += test_probs\n",
    "\n",
    "test_d1_preds /= FOLDS # Average across folds\n",
    "\n",
    "test_d1_labels = (test_d1_preds > 0.5).astype(int)\n",
    "test_d1_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall OOF accuracy with model pred: 0.9550\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_d1, oof_d1_preds)\n",
    "print(f\"Overall OOF accuracy with model pred: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       500\n",
      "           1       0.97      0.94      0.95       500\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.96      0.95      0.95      1000\n",
      "weighted avg       0.96      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_d1, oof_d1_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用 train 数据，避免信息泄漏 (data leakage)\n",
    "token_freq_d2 = Counter()\n",
    "for tokens in train_d2[\"text\"]:\n",
    "    token_freq_d2.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. 拼接所有文本\n",
    "d2_texts = pd.concat([train_d2[\"text\"], test_d2[\"text\"]], ignore_index=True)\n",
    "\n",
    "# 1. 一次性生成 n‑gram BoW\n",
    "X_d2_all = tokens_to_ngram_bow(d2_texts, vocab_size, n_list=(1,2,3), min_freq=3)\n",
    "\n",
    "# 2. 切分回 train / test\n",
    "X_d2_bow   = X_d2_all[:len(train_d2)]\n",
    "test_d2_bow = X_d2_all[len(train_d2):]\n",
    "\n",
    "# 3. Apply TF-IDF transformation\n",
    "d2_tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
    "X_train_d2_tfidf = d2_tfidf_transformer.fit_transform(X_d2_bow)\n",
    "X_test_d2_tfidf = d2_tfidf_transformer.transform(test_d2_bow)\n",
    "\n",
    "# 4. Seq length and token diversity features\n",
    "num_feats_train_d2 = build_numeric_features(train_d2[\"text\"], token_freq_d2)   # ndarray\n",
    "num_feats_test_d2  = build_numeric_features(test_d2[\"text\"],  token_freq_d2)\n",
    "\n",
    "# 5. Token frequency percentiles and variance\n",
    "train_freq_feats_d2 = build_frequency_features(train_d2[\"text\"], token_freq_d2)\n",
    "test_freq_feats_d2 = build_frequency_features(test_d2[\"text\"], token_freq_d2)\n",
    "\n",
    "# 6. Scale numerical features\n",
    "d2_num_scaler = MaxAbsScaler()\n",
    "train_num_feats_scaled_d2 = d2_num_scaler.fit_transform(num_feats_train_d2)\n",
    "test_num_feats_scaled_d2 = d2_num_scaler.transform(num_feats_test_d2)\n",
    "\n",
    "d2_freq_scaler = MaxAbsScaler()\n",
    "train_freq_feats_scaled_d2 = d2_freq_scaler.fit_transform(train_freq_feats_d2)\n",
    "test_freq_feats_scaled_d2 = d2_freq_scaler.transform(test_freq_feats_d2)\n",
    "\n",
    "# 7. Concatenate all features for train and test separately\n",
    "X_train_final_d2 = hstack([\n",
    "    X_d2_bow,\n",
    "    # X_train_d2_tfidf,\n",
    "    train_num_feats_scaled_d2,\n",
    "    # train_freq_feats_scaled_d2\n",
    "]).tocsr()\n",
    "\n",
    "X_test_final_d2 = hstack([\n",
    "    test_d2_bow,\n",
    "    # X_test_d2_tfidf,\n",
    "    test_num_feats_scaled_d2,\n",
    "    # test_freq_feats_scaled_d2\n",
    "]).tocsr()\n",
    "\n",
    "y_d2 = train_d2[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/sml/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## out of fold predictions\n",
    "oof_d2_preds = np.zeros(len(y_d2), dtype=int)\n",
    "oof_d2_probs = np.zeros(len(y_d2))\n",
    "\n",
    "test_d2_preds = np.zeros(len(test_d2))\n",
    "\n",
    "FOLDS = 20\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "for train_index, val_index in skf.split(X_train_final_d2, y_d2):\n",
    "    X_train = X_train_final_d2[train_index]\n",
    "    X_val = X_train_final_d2[val_index]\n",
    "\n",
    "    y_train = y_d2.iloc[train_index]\n",
    "    y_val = y_d2.iloc[val_index]\n",
    "\n",
    "    smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Model prediction\n",
    "    clf1 = LogisticRegression(max_iter=1000, C=0.1, solver='liblinear', random_state=42)\n",
    "    clf2 = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\", alpha=0.1, penalty='l2')\n",
    "    # clf3 = RandomForestClassifier(n_estimators=200) \n",
    "    eclf = VotingClassifier(estimators=[('lr', clf1), ('sgd', clf2)], voting='soft')\n",
    "\n",
    "    eclf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # VALIDATION: Apply TTA to validation set\n",
    "    X_val_aug_list = apply_tta(X_val, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented validation versions\n",
    "    val_probs = np.zeros(X_val.shape[0])\n",
    "    for X_val_aug in X_val_aug_list:\n",
    "        val_probs += eclf.predict_proba(X_val_aug)[:, 1]\n",
    "    val_probs /= len(X_val_aug_list)\n",
    "    \n",
    "    oof_d2_preds[val_index] = (val_probs > 0.5).astype(int)\n",
    "\n",
    "    # Apply TTA with 5 augmentations\n",
    "    X_test_aug_list = apply_tta(X_test_final_d2, n_augmentations=2)\n",
    "    \n",
    "    # Make predictions on all augmented versions\n",
    "    test_probs = np.zeros(len(test_d2))\n",
    "    for X_test_aug in X_test_aug_list:\n",
    "        test_probs += eclf.predict_proba(X_test_aug)[:, 1]\n",
    "    test_probs /= len(X_test_aug_list)\n",
    "\n",
    "    # # direct output for valication data, probability for test for mean value later\n",
    "    # oof_d2_preds[val_index] = eclf.predict(X_val)\n",
    "    # oof_d2_probs[val_index] += eclf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # use model for test prediction\n",
    "    test_d2_preds += test_probs\n",
    "\n",
    "test_d2_preds /= FOLDS # Average across folds\n",
    "\n",
    "test_d2_labels = (test_d2_preds > 0.5).astype(int)\n",
    "test_d2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall OOF accuracy with model pred: 0.9382\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy_score(y_d2, oof_d2_preds)\n",
    "print(f\"Overall OOF accuracy with model pred: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.91      0.60       250\n",
      "           1       0.99      0.94      0.97      4750\n",
      "\n",
      "    accuracy                           0.94      5000\n",
      "   macro avg       0.72      0.92      0.78      5000\n",
      "weighted avg       0.97      0.94      0.95      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_d2, oof_d2_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>text_length</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>token_diversity</th>\n",
       "      <th>domain_pred</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[9159, 3048, 238, 276, 162, 286, 305, 22, 36, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>90</td>\n",
       "      <td>0.358566</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[64, 5039, 1275, 6, 0, 871, 139, 270, 327, 237...</td>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>114</td>\n",
       "      <td>0.572864</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[327, 618, 76, 650, 121, 274, 1025, 0, 12207, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "      <td>204</td>\n",
       "      <td>0.574648</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[6, 12, 609, 11905, 4, 879, 677, 78, 13352, 60...</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>69</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 5504, 55, 22, 101, 3783, 139, 2664, 4, 1, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>144</td>\n",
       "      <td>95</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id  text_length  \\\n",
       "0  [9159, 3048, 238, 276, 162, 286, 305, 22, 36, ...   0          251   \n",
       "0  [64, 5039, 1275, 6, 0, 871, 139, 270, 327, 237...   1          199   \n",
       "1  [327, 618, 76, 650, 121, 274, 1025, 0, 12207, ...   2          355   \n",
       "1  [6, 12, 609, 11905, 4, 879, 677, 78, 13352, 60...   3          102   \n",
       "2  [1, 5504, 55, 22, 101, 3783, 139, 2664, 4, 1, ...   4          144   \n",
       "\n",
       "   unique_tokens  token_diversity  domain_pred  label  \n",
       "0             90         0.358566            1      1  \n",
       "0            114         0.572864            0      0  \n",
       "1            204         0.574648            1      0  \n",
       "1             69         0.676471            0      1  \n",
       "2             95         0.659722            0      0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d1[\"label\"] = test_d1_labels\n",
    "test_d2[\"label\"] = test_d2_labels\n",
    "\n",
    "test_preds_df = pd.concat([test_d1, test_d2])\n",
    "test_preds_df = test_preds_df.sort_values('id')\n",
    "test_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"sample.csv\")\n",
    "sub[\"class\"] = test_preds_df[\"label\"].values\n",
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
